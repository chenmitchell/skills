# Smart Context Skill ‚Äî Design Document v2

## Vision
A skill that makes any OpenClaw agent dramatically more token-efficient through smart model routing, context pruning, and prompt engineering patterns.

## Integration Points Discovered

### Model Switching (‚úÖ POSSIBLE)
- **`session_status(model=X)`** ‚Äî can set per-session model override at any time
- **`/model` command** ‚Äî switches model on the fly
- **Cron jobs** ‚Äî can set model per job
- **Sub-agents** ‚Äî can route to different models
- **`model=default`** ‚Äî resets override back to config default

### Context Control (‚ö†Ô∏è LIMITED)
- Workspace files (AGENTS.md, SOUL.md, etc.) are injected automatically by OpenClaw
- We likely can't skip them per-message
- But we CAN control what we load additionally (memory files, notes, etc.)
- Skills are loaded on-demand (only when task matches description)

### Thinking Budget (‚úÖ CONTROLLABLE)
- Thinking can be toggled: off, low, medium, high, stream
- Lower thinking = fewer tokens = cheaper
- Could match thinking level to task complexity

## Architecture ‚Äî Revised

### The Key Insight
We can't intercept BEFORE the model processes (we ARE the model). But we CAN:
1. **Self-regulate** ‚Äî recognize simple tasks and give short responses
2. **Switch models for future messages** ‚Äî but this affects the NEXT message, not current
3. **Use sub-agents with cheaper models** ‚Äî delegate simple tasks
4. **Control thinking budget** ‚Äî less thinking for simple tasks
5. **Minimize tool calls** ‚Äî batch, skip unnecessary reads

### Approach: Self-Aware Efficiency

Rather than a middleware layer, this is a **behavioral skill** ‚Äî patterns and guidelines that make the agent itself more efficient. Think of it as training, not infrastructure.

```
Message arrives ‚Üí I'm already running as Opus
                  ‚îÇ
                  ‚îú‚îÄ Simple? ‚Üí Short response, no tool calls, suggest model downgrade
                  ‚îú‚îÄ Medium? ‚Üí Normal response, efficient tool use
                  ‚îî‚îÄ Complex? ‚Üí Full response, deep thinking, all tools available
```

### What the Skill Actually Contains

#### 1. Response Sizing Guidelines
```markdown
## Response Sizing
- Yes/no question ‚Üí 1-2 sentences max
- Status check ‚Üí result only, no narration
- Simple task ‚Üí do it, confirm briefly
- Explanation request ‚Üí structured, concise
- Complex planning ‚Üí detailed with sections
- NEVER pad responses with filler
```

#### 2. Context Loading Rules
```markdown
## Context Loading
- DON'T read memory files for simple tasks (reminders, acks, status checks)
- DON'T run memory_search unless the question is about past events
- DO batch independent tool calls
- DO skip file reads when you already have the info in context
- PREFER cached/known info over re-reading files
```

#### 3. Model Routing Protocol
```markdown
## Model Routing
After each interaction, assess if the current model is right for the conversation:

### Downgrade to Sonnet when:
- Conversation is routine (status checks, simple tasks, casual chat)
- No complex reasoning needed in foreseeable messages
- Use: session_status(model="anthropic/claude-sonnet-4-5")

### Downgrade to Haiku when:
- Only doing heartbeats, simple acks, reminders
- Use: session_status(model="anthropic/claude-haiku-3-5")

### Upgrade to Opus when:
- Complex planning, architecture, debugging
- Multi-step creative work
- Important decisions that need nuance
- Use: session_status(model="anthropic/claude-opus-4-6")

### Reset:
- session_status(model="default")
```

#### 4. Thinking Budget Protocol
```markdown
## Thinking Budget
- Simple tasks ‚Üí thinking: off
- Normal tasks ‚Üí thinking: low  
- Complex reasoning ‚Üí thinking: medium/high
- Suggest /reasoning toggle when appropriate
```

#### 5. Cost Tracking
```markdown
## Cost Awareness
- Check session_status periodically to see token usage
- Track context % ‚Äî suggest compaction before hitting limits
- Note expensive operations and suggest cheaper alternatives
- Report cost when doing expensive operations (CodeLayer, long research)
```

#### 6. Sub-Agent Routing
```markdown
## Delegation
For truly simple tasks that don't need current context:
- Use sessions_spawn with cheaper model
- Good for: background research, file processing, data formatting
- Bad for: anything needing conversation context
```

## Implementation Plan

### Phase 1: Behavioral Skill (Week 1)
- Write SKILL.md with all the above patterns
- Install in OpenClaw skills directory
- Test with real conversations
- Measure token usage before/after

### Phase 2: Self-Switching (Week 2)
- Agent starts suggesting model downgrades
- Test session_status(model=X) switching
- Build the feedback loop (track costs, adapt)

### Phase 3: Automation (Week 3+)
- Automatic model switching based on message patterns
- Cron job for usage intelligence
- Dashboard for cost tracking

## Expected Impact
- **30-50% token reduction** on routine conversations
- **Faster responses** for simple tasks (less thinking, shorter output)
- **Better cost predictability** ‚Äî know what things cost
- **Foundation for Usage Intelligence skill** ‚Äî cost data feeds into optimization

## Open Questions Resolved
1. ‚úÖ Can we switch models mid-session? YES ‚Äî `session_status(model=X)`
2. ‚ö†Ô∏è Do we control workspace file injection? NO ‚Äî but we control additional reads
3. ‚ùå Can a skill act as middleware? NO ‚Äî but it can be a behavioral guide
4. üîÑ How to measure "good enough"? Track user reactions, escalation requests
5. ‚úÖ Should this be a skill or core? START as skill, propose to core later if proven
